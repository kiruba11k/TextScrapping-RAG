import streamlit as st
import validators
from langchain.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import WebBaseLoader
from langchain.retrievers import BM25Retriever
import os
from langchain_openai import ChatOpenAI
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from langgraph.graph import Graph
from langgraph.checkpoint.memory import MemorySaver

# Streamlit UI
st.title("ğŸ§ Text Scraping RAG System")

# User Input
url = st.text_input("ğŸ”— Enter Website URL for Scraping")
query = st.text_input("â“ Enter your Query")

def is_valid_url(url):
    return validators.url(url)

def scrape_and_process(url):
    """Scrapes a website, processes text, and stores embeddings in FAISS & BM25."""
    if not is_valid_url(url):
        st.warning("ğŸš¨ ENTER PROPER URL")
        return None, None
    
    loader = WebBaseLoader(web_paths=(url,))
    try:
        docs = loader.load()
    except Exception:
        st.error("âŒ Invalid URL format. Please enter a proper URL.")
        return None, None
    
    if not docs:
        st.error("ğŸ˜£ No data retrieved from the URL. Try another website.")
        return None, None

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=150)
    texts = text_splitter.split_documents(docs)

    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/multi-qa-MiniLM-L6-cos-v1")
    vector_db = FAISS.from_documents(texts, embeddings)
    vector_db.save_local("faiss_index")

    bm25_retriever = BM25Retriever.from_documents(texts)

    st.session_state["vector_db"] = vector_db
    st.session_state["bm25_retriever"] = bm25_retriever
    st.success("ğŸ¤© Data successfully scraped and indexed!")
    
    return vector_db, bm25_retriever

def retrieve_or_fallback(query):
    """Retrieves answers from RAG or falls back to LLM."""
    if "vector_db" not in st.session_state or "bm25_retriever" not in st.session_state:
        return {"response": "No indexed data found. Scrape a website first.", "source": "None"}

    vector_db = st.session_state["vector_db"]
    bm25_retriever = st.session_state["bm25_retriever"]

    retriever = vector_db.as_retriever()
    retrieved_docs = retriever.get_relevant_documents(query)
    
    if not retrieved_docs:
        retrieved_docs = bm25_retriever.get_relevant_documents(query)

    if not retrieved_docs:
        return {"response": None, "source": "LLM"}  # Signal to use LLM fallback
    
    return {"response": retrieved_docs[0].page_content, "source": "RAG"}

def llm_generate(query):
    """Generates a response using LLM if RAG fails."""
    llm = ChatGroq(model_name="Gemma2-9b-It")
    response = llm.invoke(query)
    return {"response": response, "source": "LLM"}

# LangGraph Workflow
memory = MemorySaver()
workflow = Graph()

workflow.add_node("scraper", scrape_and_process)
workflow.add_node("retriever", retrieve_or_fallback)
workflow.add_node("llm", llm_generate)

workflow.set_entry_point("scraper")
workflow.add_edge("scraper", "retriever")
workflow.add_conditional_edges("retriever", lambda x: "llm" if x["response"] is None else None)

app = workflow.compile(checkpointer=memory)

# Streamlit Sidebar
if st.sidebar.button("Scrape & Process"):
    if url:
        with st.spinner("Scraping and indexing data..."):
            vector_db, bm25_retriever = scrape_and_process(url)
            if vector_db:
                st.session_state["vector_db"] = vector_db
                st.session_state["bm25_retriever"] = bm25_retriever
    else:
        st.error("ğŸ˜£ Please enter a valid URL.")

# Query Processing
if query:
    with st.spinner("ğŸ§ Searching relevant information..."):
        retrieval_result = retrieve_or_fallback(query)
        
        if retrieval_result["source"] == "RAG":
            st.write("**ğŸ” RAG Retrieved Answer:**", retrieval_result["response"])
        else:
            llm_result = llm_generate(query)
            st.write("**ğŸ§  Generated by LLM:**", llm_result["response"])

st.sidebar.write("ğŸ«£ Built by [Kirubakaran](https://github.com/kiruba11k)")
