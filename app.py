import streamlit as st
import os
import validators
from langchain.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import WebBaseLoader
from langchain.retrievers import BM25Retriever
from langchain_openai import ChatOpenAI
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from langgraph.graph import Graph
from langgraph.checkpoint.memory import MemorySaver

# Load API Key from Streamlit Secrets
GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
os.environ["GROQ_API_KEY"] = GROQ_API_KEY

# Streamlit UI
st.set_page_config(page_title="Web Scraper RAG", page_icon="ü§ó", layout="wide")
st.title("Text Scraping RAG System")

url = st.sidebar.text_input("Enter website URL:")
query = st.text_input("Enter your query:")

def is_valid_url(url):
    """Check if the URL is valid."""
    return validators.url(url)

def scrape_and_process(url):
    """Scrapes data from the website, processes it, and indexes it in FAISS."""
    if not is_valid_url(url):
        st.warning("üö® ENTER PROPER URL")
        return None, None

    loader = WebBaseLoader(web_paths=(url,))
    docs = loader.load()
    
    if not docs:
        st.error("üò£ No data retrieved from the URL. Try another website.")
        return None, None

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=150)
    texts = text_splitter.split_documents(docs)
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/multi-qa-MiniLM-L6-cos-v1")

    vector_db = FAISS.from_documents(texts, embeddings)
    vector_db.save_local("faiss_index")
    bm25_retriever = BM25Retriever.from_documents(texts)

    st.session_state["vector_db"] = vector_db
    st.session_state["bm25_retriever"] = bm25_retriever
    st.success("ü§© Data successfully scraped and indexed!")

    return vector_db, bm25_retriever

def retrieve_docs(query):
    """Retrieves relevant documents from FAISS or BM25."""
    vector_db = st.session_state.get("vector_db")
    bm25_retriever = st.session_state.get("bm25_retriever")
    
    if not vector_db or not bm25_retriever:
        return None, "üëª No retrievers found. Please scrape data first."
    
    retriever = vector_db.as_retriever()
    retrieved_docs = retriever.get_relevant_documents(query)
    
    if not retrieved_docs:
        retrieved_docs = bm25_retriever.get_relevant_documents(query)
    
    return retrieved_docs, None

def generate_llm_response(query):
    """Generates a response using LLM when no relevant documents are found."""
    llm = ChatGroq(model_name="Gemma2-9b-It")
    return llm.invoke(query)

# Define workflow with conditional edges
memory = MemorySaver()
workflow = Graph()
workflow.add_node("scraper", scrape_and_process)
workflow.add_node("retriever", retrieve_docs)
workflow.add_node("llm", generate_llm_response)

# Conditional edges: Use RAG if possible, otherwise fallback to LLM
workflow.add_edge("scraper", "retriever")
workflow.add_conditional_edges("retriever", lambda data: "llm" if not data[0] else None)
workflow.add_edge("llm", None)  # LLM final response

app = workflow.compile(checkpointer=memory)

if st.sidebar.button("Scrape & Process"):
    if url:
        with st.spinner("Scraping and indexing data..."):
            vector_db, bm25_retriever = scrape_and_process(url)
            if vector_db:
                st.session_state["vector_db"] = vector_db
                st.session_state["bm25_retriever"] = bm25_retriever
    else:
        st.error("üò£ Please enter a valid URL.")

if query:
    with st.spinner("üßê Searching relevant information..."):
        retrieved_docs, error = retrieve_docs(query)
        if error:
            st.error(error)
        elif retrieved_docs:
            st.write("**RAG Retrieved Answer:**", retrieved_docs[0].page_content)
        else:
            llm_response = generate_llm_response(query)
            st.write("**Generated by LLM:**", llm_response)

st.sidebar.write("ü´£ Built by [Kirubakaran](https://github.com/kiruba11k)")
